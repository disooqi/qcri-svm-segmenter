{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from itertools import izip, combinations\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the CSV file from original word, Farasa and ARZ segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://docs.python.org/2/library/csv.html\n",
    "\n",
    "import csv, codecs, cStringIO\n",
    "\n",
    "class UTF8Recoder:\n",
    "    \"\"\"\n",
    "    Iterator that reads an encoded stream and reencodes the input to UTF-8\n",
    "    \"\"\"\n",
    "    def __init__(self, f, encoding):\n",
    "        self.reader = codecs.getreader(encoding)(f)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        return self.reader.next().encode(\"utf-8\")\n",
    "\n",
    "class UnicodeReader:\n",
    "    \"\"\"\n",
    "    A CSV reader which will iterate over lines in the CSV file \"f\",\n",
    "    which is encoded in the given encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
    "        f = UTF8Recoder(f, encoding)\n",
    "        self.reader = csv.reader(f, dialect=dialect, **kwds)\n",
    "\n",
    "    def next(self):\n",
    "        row = self.reader.next()\n",
    "        return [unicode(s, \"utf-8\") for s in row]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "class UnicodeWriter:\n",
    "    \"\"\"\n",
    "    A CSV writer which will write rows to CSV file \"f\",\n",
    "    which is encoded in the given encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
    "        # Redirect output to a queue\n",
    "        self.queue = cStringIO.StringIO()\n",
    "        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n",
    "        self.stream = f\n",
    "        self.encoder = codecs.getincrementalencoder(encoding)()\n",
    "\n",
    "    def writerow(self, row):\n",
    "        self.writer.writerow([s.encode(\"utf-8\") for s in row])\n",
    "        # Fetch UTF-8 output from the queue ...\n",
    "        data = self.queue.getvalue()\n",
    "        data = data.decode(\"utf-8\")\n",
    "        # ... and reencode it into the target encoding\n",
    "        data = self.encoder.encode(data)\n",
    "        # write to the target stream\n",
    "        self.stream.write(data)\n",
    "        # empty queue\n",
    "        self.queue.truncate(0)\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ليه\n",
      "7077 3479\n"
     ]
    }
   ],
   "source": [
    "segmentations = dict()\n",
    "all_tokens = list()\n",
    "with open('new_data/Dialects Tokens Annotation - EGY.csv', mode='rb') as csvf:\n",
    "    reader = UnicodeReader(csvf, delimiter=',')#, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i, x in enumerate(reader):\n",
    "        \n",
    "        if x[7] == '#':\n",
    "            hashtag = list()\n",
    "            hash_flag = True\n",
    "            hashtag.append(x[7])\n",
    "            \n",
    "        elif hash_flag and x[7] == '_':\n",
    "            hashtag.append(x[7])\n",
    "        elif hash_flag and x[7] != '_':\n",
    "            hash_flag = False\n",
    "            hashtag.append(x[7])\n",
    "            del hashtag\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        if i==0:\n",
    "            print x[7]\n",
    "#         if x[4].strip():\n",
    "#             segmentations[x[4]] = x[7]\n",
    "            \n",
    "    else:\n",
    "#         segmentations[u'8.'] = u'8 .'\n",
    "#         segmentations[u','] = u','\n",
    "#         segmentations[u'الجنَّة'] = u'ال+جن+ة'\n",
    "#         segmentations[u'اممممممممم'] = u'اممممممممم'\n",
    "#         segmentations[u'مليار'] = u'مليار'\n",
    "#         segmentations[u'القمح'] = u'ال+قمح'\n",
    "#         segmentations[u'المستوردين'] = u'ال+مستورد+ين'\n",
    "#         segmentations[u'9'] = u'9'\n",
    "#         segmentations[u'3'] = u'3'\n",
    "#         segmentations[u'مابين'] = u'مابين'\n",
    "#         segmentations[u'ولي'] = u'ولي'\n",
    "#         segmentations[u'وارباب'] = u'و+ارباب'\n",
    "#         segmentations[u'العمل'] = u'ال+عمل'\n",
    "#         segmentations[u'كايتقام'] = u'كا+يتقام'\n",
    "#         segmentations[u'كايستافدو'] = u'كا+يستافد+و'\n",
    "#         segmentations[u'تستقر'] = u'تستقر'\n",
    "#         segmentations[u'المقاصة'] = u'ال+مقاص+ة'\n",
    "#         segmentations[u'لصندوق'] = u'ل+صندوق'\n",
    "#         segmentations[u'الفلاحية'] = u'ال+فلاحي+ة'\n",
    "#         segmentations[u'السنة'] = u'ال+سن+ة'\n",
    "#         segmentations[u'\\u0627\\u0644\\u062f\\u0648\\u0644'] = u'\\u0627\\u0644\\u062f\\u0648\\u0644'\n",
    "#         segmentations[u'\\u0648\\u062a\\u0633\\u0627\\u0641\\u0631'] = u'\\u0648\\u062a\\u0633\\u0627\\u0641\\u0631'\n",
    "#         segmentations[u'\\u0627\\u0644\\u0642\\u0645\\u0629'] = u'\\u0627\\u0644\\u0642\\u0645\\u0629'\n",
    "#         segmentations[u'\\u0639\\u0627\\u0644\\u064a\\u0629'] = u'\\u0639\\u0627\\u0644\\u064a\\u0629'\n",
    "#         segmentations[u'\\u0627\\u062d\\u0633\\u0627\\u0633\\u0643'] = u'\\u0627\\u062d\\u0633\\u0627\\u0633\\u0643'\n",
    "#         segmentations[u'\\u0634\\u062a\\u0642\\u062a\\u0644\\u0647\\u0627'] = u'\\u0634\\u062a\\u0642\\u062a\\u0644\\u0647\\u0627'\n",
    "        print i+1, len(segmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pAllDiacritics = re.compile(u\"[\\u0640\\u064b\\u064c\\u064d\\u064e\\u064f\\u0650\\u0651\\u0652\\u0670]\", re.U)\n",
    "pAllNonCharacters = re.compile(u\"[\\u0020\\u2000-\\u200F\\u2028-\\u202F\\u205F-\\u206F\\uFEFF]+\", re.U)\n",
    "emailRegex = re.compile(\"[a-zA-Z0-9\\\\-\\\\._]+@[a-zA-Z0-9\\\\-\\\\._]+$\", re.U)\n",
    "ALLDelimiters = u\"\\u0020\\u0000-\\u002F\\u003A-\\u0040\\u007B-\\u00BB\\u005B-\\u005D\\u005F-\\u0060\\\\^\\u0600-\\u060C\\u06D4-\\u06ED\\ufeff\"\n",
    "pAllDelimiters = re.compile(u\"[\" + ALLDelimiters + \"]+\", re.U)\n",
    "AllArabicLettersAndHindiDigits = u\"\\u0621\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062A\\u062B\\u062C\\u062D\\u062E\\u062F\" \\\n",
    "                                 + u\"\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063A\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064A\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669\"\n",
    "AllDigits = u\"0123456789\"   \n",
    "AllArabicDiacritics = u\"\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652\"\n",
    "# ALL Arabic letters \\U0621-\\U063A\\U0641-\\U064A\n",
    "AllArabicLetters = u\"\\u0621\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062A\\u062B\\u062C\\u062D\\u062E\\u062F\" \\\n",
    "                   + u\"\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063A\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064A\"\n",
    "\n",
    "def removeNonCharacters(s):\n",
    "    return pAllNonCharacters.sub(' ', s)\n",
    "\n",
    "def removeDiacritics(s):\n",
    "    return pAllDiacritics.sub('', s)\n",
    "\n",
    "\n",
    "\n",
    "def charBasedTonkenizer(s):\n",
    "    sFinal = \"\"\n",
    "    for i, ch in enumerate(s):\n",
    "        if pAllDelimiters.match(s[i:i + 1]) and s[i:i + 1] != \".\" and s[i:i + 1] != \",\" and s[i:i + 1] != \".\" and s[\n",
    "                                                                                                                  i:i + 1] != \"%\":\n",
    "            sFinal += \" \" + s[i: i + 1] + \" \"\n",
    "        elif s[i: i + 1] == \"%\":\n",
    "            if i > 0 and s[i:i + 1] in AllDigits:\n",
    "                sFinal += s[i:i + 1] + \" \"\n",
    "            else:\n",
    "                sFinal += \" \" + s[i:i + 1] + \" \"\n",
    "        elif s[i:i + 1] == \".\" or s[i:i + 1] == \",\" or s[i:i + 1] == \".\":\n",
    "            if 0 < i < len(s) - 1 and s[i - 1: i] in AllDigits and s[i + 1: i + 2] in AllDigits:\n",
    "                sFinal += s[i: i + 1]\n",
    "            elif i > 0 and s[i: i + 1] == \".\" and s[i - 1: i] == \".\":\n",
    "                sFinal += s[i: i + 1]\n",
    "            elif i == 0:\n",
    "                sFinal += s[i: i + 1] + \" \"\n",
    "            elif i == len(s) - 1:\n",
    "                sFinal += \" \" + s[i: i + 1]\n",
    "            elif s[i - 1:i] in AllDigits and s[i + 1:i + 2] in AllDigits:\n",
    "                sFinal += s[i: i + 1]\n",
    "            else:\n",
    "                sFinal += \" \" + s[i: i + 1] + \" \"\n",
    "        elif s[i:i + 1] not in AllArabicLettersAndHindiDigits + u\"\\u0640\\u064b\\u064c\\u064d\\u064e\\u064f\\u0650\\u0651\\u0652\\u0670\" + u\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ÀÁÂÃÄÅÆÇÈÉËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿ\":\n",
    "            sFinal += \" \" + s[i:i+1] + \" \"\n",
    "        else:\n",
    "            if (i == 0):\n",
    "                sFinal += s[i: i + 1]\n",
    "            else:\n",
    "                if (s[i:i + 1] in AllDigits and s[i - 1: i] in (AllArabicLetters + AllArabicDiacritics)) or (\n",
    "                                s[i - 1:i] in AllDigits and s[i:i + 1] in (AllArabicLetters + AllArabicDiacritics)):\n",
    "                    sFinal += \" \" + s[i: i + 1]\n",
    "                else:\n",
    "                    sFinal += s[i: i + 1]\n",
    "\n",
    "    return sFinal.strip()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    s = removeNonCharacters(text)\n",
    "    s = removeDiacritics(s)\n",
    "\n",
    "    tnr_pattern = re.compile(u'[\\t\\n\\r]')\n",
    "    s = tnr_pattern.sub(' ', s)\n",
    "    output = list()\n",
    "    words = s.split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.startswith((u\"#\", u\"@\", u\":\", u\";\", u\"http\")) or emailRegex.match(word):\n",
    "            output.append(word)\n",
    "        else:\n",
    "            for ss in charBasedTonkenizer(word).split():\n",
    "                if len(ss.strip()) > 0:\n",
    "                    if ss.startswith(u\"\\u0644\\u0644\"):\n",
    "                        output.append(u\"\\u0644\\u0627\\u0644\" + ss[2:])\n",
    "                    else:\n",
    "                        if len(ss.strip()) > 0:\n",
    "                            output.append(ss)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(ur'#\\s[\\w\\+]+(\\s_\\s[\\w\\+]+)*', re.U)\n",
    "def remove_spaces(match):\n",
    "    return match.group().replace(' ', '').replace('+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/lev_seg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/lev_seg/splits\n",
      "4986 \tlev_trainfold_04.trg\n",
      "674 \tlev_devfold_02.trg\n",
      "5591 \tlev_trainfold_02.trg\n",
      "1308 \tlev_testfold_01.trg\n",
      "4991 \tlev_trainfold_01.trg\n",
      "674 \tlev_devfold_04.trg\n",
      "1451 \tlev_testfold_05.trg\n",
      "1338 \tlev_testfold_04.trg\n",
      "1494 \tlev_testfold_03.trg\n",
      "4897 \tlev_trainfold_05.trg\n",
      "674 \tlev_devfold_03.trg\n",
      "650 \tlev_devfold_05.trg\n",
      "1407 \tlev_testfold_02.trg\n",
      "699 \tlev_devfold_01.trg\n",
      "4830 \tlev_trainfold_03.trg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/lev_seg/splits_msa\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/DATA_SEG_All_27.03.2017\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/magh_seg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/magh_seg/splits\n",
      "1137 \tdata_5.test.trg\n",
      "1102 \tdata_2.test.trg\n",
      "1224 \tdata_1.test.trg\n",
      "4036 \tdata_4.train.trg\n",
      "632 \tdata_3.dev.trg\n",
      "4159 \tdata_2.train.trg\n",
      "1134 \tdata_3.test.trg\n",
      "584 \tdata_2.dev.trg\n",
      "562 \tdata_4.dev.trg\n",
      "1247 \tdata_4.test.trg\n",
      "598 \tdata_5.dev.trg\n",
      "501 \tdata_1.dev.trg\n",
      "4079 \tdata_3.train.trg\n",
      "4110 \tdata_5.train.trg\n",
      "4120 \tdata_1.train.trg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/magh_seg/splits_msa\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/egy_seg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/egy_seg/splits\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/egy_seg/splits_msa\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/gulf_seg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/gulf_seg/splits\n",
      "1342 \tglf_testfold_02.trg\n",
      "1269 \tglf_testfold_04.trg\n",
      "1283 \tglf_testfold_01.trg\n",
      "1316 \tglf_testfold_05.trg\n",
      "661 \tglf_devfold_01.trg\n",
      "661 \tglf_devfold_03.trg\n",
      "661 \tglf_devfold_04.trg\n",
      "622 \tglf_devfold_05.trg\n",
      "661 \tglf_devfold_02.trg\n",
      "5285 \tglf_trainfold_01.trg\n",
      "5299 \tglf_trainfold_04.trg\n",
      "1358 \tglf_testfold_03.trg\n",
      "5210 \tglf_trainfold_03.trg\n",
      "4630 \tglf_trainfold_05.trg\n",
      "5226 \tglf_trainfold_02.trg\n",
      "Found directory: ../../qcri/egy_seg/DATA_SEG_All_27.03.2017/gulf_seg/splits_msa\n"
     ]
    }
   ],
   "source": [
    "# Set the directory you want to start from\n",
    "rootDir = '../../qcri/egy_seg/DATA_SEG_All_27.03.2017/'\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    print('Found directory: %s' % dirName)\n",
    "    for fname in fileList:\n",
    "        if fname.endswith('.trg') and os.path.basename(dirName)== 'splits' and os.path.basename(os.path.dirname(dirName))!= 'egy_seg':\n",
    "            all_tokens = list()\n",
    "            with codecs.open(os.path.join(dirName,fname), encoding='utf8') as rfobj:\n",
    "                \n",
    "                for line in rfobj:\n",
    "                    if line.strip() != \"<EOTWEET>\":\n",
    "                        all_tokens.append(line.strip())\n",
    "                    else:\n",
    "                        all_tokens.append('EOTWEET')\n",
    "                else:\n",
    "                    dial_dir = os.path.basename(os.path.dirname(dirName))\n",
    "                    with codecs.open(os.path.join('./new_data/',dial_dir,fname[:-3]+'svm'), encoding='utf8', mode='w') as rfobj:\n",
    "                        single_line = ' '.join(all_tokens)\n",
    "                        if os.path.basename(os.path.dirname(dirName))== 'egy_seg':\n",
    "                            single_line = p.sub(remove_spaces, single_line)\n",
    "                        tokenized = ' '.join(tokenize(single_line))\n",
    "                        for tk in tokenized.replace(\" + \",\"+\").split():                            \n",
    "                            if os.path.basename(os.path.dirname(dirName)) != 'egy_seg':                                \n",
    "                                rfobj.write(tk.replace('+', ''))\n",
    "                                rfobj.write('\\t')\n",
    "                            rfobj.write(tk)\n",
    "                            rfobj.write('\\n')\n",
    "                        \n",
    "                    \n",
    "                    print len(all_tokens),\n",
    "            print('\\t%s' % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: new_data/orig4desouki/\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "1463 \tdata_5.test.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "1415 \tdata_2.test.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "1421 \tdata_1.test.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "4975 \tdata_4.train.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "735 \tdata_3.dev.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "4928 \tdata_2.train.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "1380 \tdata_3.test.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "734 \tdata_2.dev.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "705 \tdata_4.dev.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "1397 \tdata_4.test.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "805 \tdata_5.dev.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "579 \tdata_1.dev.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "4962 \tdata_3.train.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "4809 \tdata_5.train.trg\n",
      "new_data/orig4desouki/\n",
      "new_data\n",
      "5077 \tdata_1.train.trg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "# Set the directory you want to start from\n",
    "rootDir = 'new_data/orig4desouki/'\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    print('Found directory: %s' % dirName)\n",
    "    for fname in fileList:\n",
    "        if fname.endswith('.trg'): #and os.path.basename(dirName)== 'splits' and os.path.basename(os.path.dirname(dirName))!= 'egy_seg':\n",
    "            all_tokens = list()\n",
    "            with codecs.open(os.path.join(dirName,fname), encoding='utf8') as rfobj:                \n",
    "                for line in rfobj:\n",
    "                    all_tokens.append(line.strip())\n",
    "                else:\n",
    "                    print dirName\n",
    "                    print os.path.dirname(os.path.dirname(dirName))\n",
    "                    dial_dir = os.path.basename(os.path.dirname(dirName))\n",
    "                    with codecs.open(os.path.join(os.path.dirname(os.path.dirname(dirName)),'egy_new_data',fname[:-3]+'src'), encoding='utf8', mode='w') as rfobj:\n",
    "                        single_line = ' '.join(all_tokens)\n",
    "                        single_line = p.sub(remove_spaces, single_line)\n",
    "                        tokenized = ' '.join(tokenize(single_line))\n",
    "                        for tk in tokenized.replace(\" + \",\"\").split():                            \n",
    "                            rfobj.write(tk)\n",
    "                            rfobj.write('\\n')\n",
    "                        \n",
    "                    \n",
    "                    print len(all_tokens),\n",
    "            print('\\t%s' % fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dev and train files for EGY and MAGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: final_splits_all_data/joint/joint\n",
      "magh_5.train.trg\n",
      "magh_3.train.trg\n",
      "magh_1.train.trg\n",
      "magh_4.train.trg\n",
      "magh_2.train.trg\n",
      "Found directory: final_splits_all_data/joint/joint/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# rootDir = 'final_splits_all_data/joint/joint/'\n",
    "rootDir = 'final_splits_all_data/joint/joint'\n",
    "for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "    print('Found directory: %s' % dirName)\n",
    "#     print os.path.dirname(dirName)\n",
    "#     print os.path.basename(os.path.dirname(dirName))\n",
    "#     print os.path.basename(dirName)\n",
    "    for fname in fileList:\n",
    "        if os.path.basename(dirName) == 'joint' and fname.startswith('magh_') and 'train' in fname: #and os.path.basename(dirName)== 'splits' and os.path.basename(os.path.dirname(dirName))!= 'egy_seg':\n",
    "            print fname\n",
    "            with codecs.open(os.path.join(dirName,fname[:-4]), encoding='utf8', mode='w') as rfobj:\n",
    "                with codecs.open(os.path.join(dirName,fname), encoding='utf8') as f1obj:\n",
    "                    for line in f1obj:\n",
    "#                         rfobj.write(line.strip().replace('+', ''))\n",
    "#                         rfobj.write('\\t')\n",
    "                        rfobj.write(line)\n",
    "                with codecs.open(os.path.join(dirName,fname.replace('train','dev')), encoding='utf8') as f1obj:\n",
    "#                     # lev_devfold_04.trg\n",
    "                    for line in f1obj:\n",
    "#                         rfobj.write(line.strip().replace('+', ''))\n",
    "#                         rfobj.write('\\t')\n",
    "                        rfobj.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
